{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1BfaTQ9kxHjn2UawQ2e9M4uqAn400HjFx",
      "authorship_tag": "ABX9TyMHhU/SR8z9467io6pFgl8p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Linkanblomman/Fight_recognition/blob/master/train_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzuZ9cegsPqE",
        "colab_type": "text"
      },
      "source": [
        "Connect to seperate folder that have been loaded into Google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb-lp7DksEmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change the example path \"/content/drive/My Drive/Colab_Notebooks/fight_recognition/\" to the downloaded project folder\n",
        "!ln -s \"/content/drive/My Drive/Colab_Notebooks/fight_recognition/\" /content/fight_recognition"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRyHM9vBt3qV",
        "colab_type": "text"
      },
      "source": [
        "Install decord for video slicing (https://github.com/dmlc/decord)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1Qh93L9sqG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install decord"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtbkeJhXskQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import math\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "matplotlib.style.use('ggplot')\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from decord import VideoReader\n",
        "from decord import bridge\n",
        "#from decord import cpu, gpu\n",
        "\n",
        "from fight_recognition.SGDR import CosineAnnealingLR_with_Restart\n",
        "\n",
        "import fight_recognition.model as ResNet\n",
        "from fight_recognition.spatial_transforms import (Compose, Normalize, Resize, CenterCrop,\n",
        "                                CornerCrop, MultiScaleCornerCrop,\n",
        "                                RandomResizedCrop, RandomHorizontalFlip,\n",
        "                                ToTensor, ScaleValue, ColorJitter,\n",
        "                                PickFirstChannels)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xohgHRR5Y7OO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_value = 42\n",
        "torch.manual_seed(seed_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzCca4qO4tLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7RCrM5yuL5C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c0dcdc0d-6eb5-49c2-d60f-0cfb324bda70"
      },
      "source": [
        "batch_size = 16\n",
        "model_architecture = 50 # Generate resnet model\n",
        "\n",
        "# K - Kinetics-700\n",
        "# KM - Kinetics-700 and Moments in Time\n",
        "dataset = 'K'\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "print(f\"Computation device: {device}\\n\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computation device: cuda:0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg8kdA8QuMfk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "2bb54574-902e-4a91-a18d-f8372c783380"
      },
      "source": [
        "# read the data.csv file and get the video paths and labels\n",
        "df = pd.read_csv('./fight_recognition/input/data.csv')\n",
        "X = df.video_path.values # video paths\n",
        "y = df.target.values # targets\n",
        "\n",
        "(xtrain, xtest, ytrain, ytest) = train_test_split(X, y, test_size=0.20, random_state=seed_value)\n",
        "\n",
        "print(f\"Training videos: {len(xtrain)}\")\n",
        "print(f\"Training labels: {len(ytrain)}\\n\")\n",
        "\n",
        "print(f\"Validation videos: {len(xtest)}\")\n",
        "print(f\"Validation labels: {len(ytest)}\\n\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training videos: 240\n",
            "Training labels: 240\n",
            "\n",
            "Validation videos: 60\n",
            "Validation labels: 60\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuWd9Sy7uko-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# custom dataset\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, videos, labels=None, spatial_transform=None):\n",
        "        self.X = videos\n",
        "        self.y = labels\n",
        "        self.spatial_transform = spatial_transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return (len(self.X))\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        vr = VideoReader(self.X[i]) # Read video\n",
        "        bridge.set_bridge('native')\n",
        "        duration = len(vr)\n",
        "        frames = 16\n",
        "        steps = math.floor(duration/frames)\n",
        "        start_frame = 0\n",
        "        stop_frame = steps * frames\n",
        "        \n",
        "        frame_id_list = range(start_frame, stop_frame, steps) \n",
        "        \n",
        "        clip = []\n",
        "        video_snippet = vr.get_batch(frame_id_list).asnumpy() # Will get a batch of 16 frames from video\n",
        "\n",
        "        # Transform into images\n",
        "        for img in video_snippet:\n",
        "            im_pil = Image.fromarray(img)\n",
        "            clip.append(im_pil)\n",
        "\n",
        "        if self.spatial_transform is not None:\n",
        "            clip = [self.spatial_transform(img) for img in clip]\n",
        "\n",
        "        clip = torch.stack(clip, 0).permute(1, 0, 2, 3)\n",
        "   \n",
        "        label = self.y[i]\n",
        "        \n",
        "        return (clip.clone().detach().requires_grad_(True), torch.tensor(label, dtype=torch.long))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YjsoeCQ4sk2",
        "colab_type": "text"
      },
      "source": [
        "Mean and std https://github.com/kenshohara/3D-ResNets-PyTorch/blob/master/main.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN1FGJjku1hY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if (dataset == 'K'):\n",
        "  mean = [0.4345, 0.4051, 0.3775]\n",
        "  std = [0.2768, 0.2713, 0.2737]\n",
        "else:\n",
        "  mean = [0.5, 0.5, 0.5]\n",
        "  std = [0.5, 0.5, 0.5]\n",
        "\n",
        "sample_size = 112 # resolution of frame\n",
        "\n",
        "# MultiScaleCornerCrop (four-corner cropping)\n",
        "scales = [1.0]\n",
        "scale_step = 1 / (2**(1 / 4))\n",
        "for _ in range(1, 5):\n",
        "    scales.append(scales[-1] * scale_step)\n",
        "\n",
        "spatial_transform_train = Compose([\n",
        "                                   MultiScaleCornerCrop(sample_size, scales),\n",
        "                                   RandomHorizontalFlip(),\n",
        "                                   ToTensor(),\n",
        "                                   Normalize(mean, std)\n",
        "                                   ])\n",
        "\n",
        "spatial_transform_validation =  Compose([Resize(sample_size),\n",
        "                                        CenterCrop(sample_size),\n",
        "                                        ToTensor(),\n",
        "                                        Normalize(mean, std)])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR7kaF6QvJfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = VideoDataset(xtrain, ytrain, spatial_transform_train)\n",
        "test_data = VideoDataset(xtest, ytest, spatial_transform_validation)\n",
        "\n",
        "TrainLoader = DataLoader(train_data, batch_size=batch_size, shuffle=True) # If shuffle is set to True, it will have the data reshuffled at every epoch\n",
        "TestLoader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMQxf76CLX3v",
        "colab_type": "text"
      },
      "source": [
        "Download pre-trained models (https://github.com/kenshohara/3D-ResNets-PyTorch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMt6tGP6vgq1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "fffc7b98-c90a-4f28-ab8d-ccefe1aeee8a"
      },
      "source": [
        "model = ResNet.initialize_model(model_architecture=model_architecture, model_dataset=dataset, num_classes=2)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: ResNet34\n",
            "Dataset: Kinetics-700\n",
            "\n",
            "Model parameters\n",
            "Learning rate: 3.0000000000000012e-09\n",
            "Momentum: 0.9\n",
            "Weight_decay: 0.0001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeYGBOIKwlm2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "e327c5a2-0d64-450f-8493-760a65b4f2f7"
      },
      "source": [
        "for name, child in model.named_children():\n",
        "    if name in ['layer4','fc']: # Layer that will be unfrozen\n",
        "        print(name + ' is unfrozen')\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = True\n",
        "    else:\n",
        "        print(name + ' is frozen')\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = False  \n",
        "\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.SGD([{'params': model.layer4.parameters()}, \n",
        "                      {'params': model.fc.parameters(), 'lr': 3e-3}\n",
        "                       ], lr=3e-05,momentum=.9, weight_decay=.0001)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# SGDR\n",
        "t_mult = 1 # cycle multiplication\n",
        "t_max = 25 # Maximum number of iterations/epochs\n",
        "scheduler = CosineAnnealingLR_with_Restart(optimizer, T_max=t_max, T_mult=t_mult, model=model, out_dir='./fight_recognition/outputs/snapshots/', take_snapshot=True, eta_min=3e-09) # eta_min – Minimum learning rate"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1 is frozen\n",
            "bn1 is frozen\n",
            "relu is frozen\n",
            "maxpool is frozen\n",
            "layer1 is frozen\n",
            "layer2 is frozen\n",
            "layer3 is frozen\n",
            "layer4 is unfrozen\n",
            "avgpool is frozen\n",
            "fc is unfrozen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BNBrCdBB3jT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# learning rate for each layer\n",
        "for param_group in optimizer.param_groups:\n",
        "    print(param_group['lr'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG9mEKVq_1dg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwmUW2ED3Jmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for n, p in model.named_parameters():\n",
        "  print(p.device, \" \", n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B49zrcFx0d1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Check model requires_grad params\\n\")\n",
        "print(\"Status\\tParameters\\n\")\n",
        "for n, p in model.named_parameters():\n",
        "    print(p.requires_grad, \" \", n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4pYVa-5yBLN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4d62b73f-7b96-477a-d249-fedcbca81623"
      },
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"{total_params:,} total parameters.\")\n",
        "total_trainable_params = sum(\n",
        "    p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"{total_trainable_params:,} training parameters.\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63,514,562 total parameters.\n",
            "39,067,650 training parameters.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYpESWIFyEPA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training function\n",
        "def fit(model, train_dataloader):\n",
        "    print('Training')\n",
        "    model.train() # training mode activated if no_grad() have deactivate the gradient calculation part in validation function\n",
        "    train_running_loss = 0.0\n",
        "    train_running_correct = 0\n",
        "    for i, data in tqdm(enumerate(train_dataloader), total=int(len(train_data)/train_dataloader.batch_size)):\n",
        "        data, target = data[0].to(device), data[1].to(device) \n",
        "        optimizer.zero_grad() # Reset optimizer to zero otherwise it will just accumulate all the gradients\n",
        "        outputs = model(data) # Input the bathed images to the model to get a output (prediction)\n",
        "        \n",
        "        # From the loss function we will get back a loss tensor. PyTorch have the computaional graph for the tensor that will be used in the backpropagation step \n",
        "        loss = criterion(outputs, target) # calculate the loss from the loss/error function (prediction_label - true_label)\n",
        "        train_running_loss += loss.item() # new loss value to update the current training loss value\n",
        "        _, preds = torch.max(input=outputs.data, dim=1) # Returns the maximum value of all elements in the input tensor\n",
        "        train_running_correct += (preds == target).sum().item() # Count the right numbers of correct prediction\n",
        "        loss.backward() # Calculate gradients\n",
        "        optimizer.step() # Update the weights\n",
        "        \n",
        "    train_loss = train_running_loss/len(train_dataloader.dataset)\n",
        "    train_accuracy = 100. * train_running_correct/len(train_dataloader.dataset)\n",
        "    \n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}\")\n",
        "    \n",
        "    return train_loss, train_accuracy"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkMXeuejyF8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#validation function\n",
        "def validate(model, test_dataloader):\n",
        "    print('Validating')\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    val_running_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for i, data in tqdm(enumerate(test_dataloader), total=int(len(test_data)/test_dataloader.batch_size)):\n",
        "            data, target = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, target)\n",
        "            \n",
        "            val_running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            val_running_correct += (preds == target).sum().item()\n",
        "        \n",
        "        val_loss = val_running_loss/len(test_dataloader.dataset)\n",
        "        val_accuracy = 100. * val_running_correct/len(test_dataloader.dataset)\n",
        "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}')\n",
        "        \n",
        "        return val_loss, val_accuracy"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4XWmsyeyIsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss , train_accuracy = [], []\n",
        "val_loss , val_accuracy = [], []\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    scheduler.step() # SGDR\n",
        "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
        "    train_epoch_loss, train_epoch_accuracy = fit(model, TrainLoader)\n",
        "    val_epoch_loss, val_epoch_accuracy = validate(model, TestLoader)\n",
        "    train_loss.append(train_epoch_loss)\n",
        "    train_accuracy.append(train_epoch_accuracy)\n",
        "    val_loss.append(val_epoch_loss)\n",
        "    val_accuracy.append(val_epoch_accuracy)\n",
        "end = time.time()\n",
        "\n",
        "print(f\"{(end-start)/60:.3f} minutes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEXGpIr1yL6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# accuracy plots\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(train_accuracy, color='green', label='train accuracy')\n",
        "plt.plot(val_accuracy, color='blue', label='validataion accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.savefig('./fight_recognition/outputs/accuracy_3DCNN.png')\n",
        "plt.show()\n",
        "\n",
        "# loss plots\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(train_loss, color='orange', label='train loss')\n",
        "plt.plot(val_loss, color='red', label='validataion loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.savefig('./fight_recognition/outputs/loss_3DCNN.png')\n",
        "plt.show()\n",
        "\n",
        "# serialize the model to disk\n",
        "print('Saving model...')\n",
        "torch.save(model.state_dict(), \"./fight_recognition/outputs/fight_reco_3DCNNmodel.pth\")\n",
        " \n",
        "print('TRAINING COMPLETE')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}