{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hyperparameter_testing.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1ngTELRyNLs-xARFCOokmNimKGt_uwQOS",
      "authorship_tag": "ABX9TyMD5jHPrbaTp1V/R+0kLy0n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Linkanblomman/Fight_recognition/blob/master/hyperparameter_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnbdLIy6Kk7R",
        "colab_type": "text"
      },
      "source": [
        "Get access to files on computer (mount drive)\n",
        "\n",
        "Easy access to files from \"content/fight_recognition\" folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuz9pOs6Ki7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example, your Google drive folder:\"/content/drive/My Drive/Colab_Notebooks/fight_recognition/\" \n",
        "# Example, colab folder: /content/fight_recognition\n",
        "!ln -s \"/content/drive/My Drive/Colab_Notebooks/fight_recognition/\" /content/fight_recognition\n",
        "# If incorrect folder and you want to reset colab: Runtime -> Factory reset runtime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qk6uFq5z3yrT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install decord"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IlnwAV45Mo_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhHY74Va31C4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import datetime, os\n",
        "import time\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import math\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "matplotlib.style.use('ggplot')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from decord import VideoReader\n",
        "from decord import bridge\n",
        "\n",
        "from fight_recognition.SGDR import CosineAnnealingLR_with_Restart\n",
        "\n",
        "import fight_recognition.model as ResNet\n",
        "from fight_recognition.spatial_transforms import (Compose, Normalize, Resize, CenterCrop,\n",
        "                                CornerCrop, MultiScaleCornerCrop,\n",
        "                                RandomResizedCrop, RandomHorizontalFlip,\n",
        "                                ToTensor, ScaleValue, ColorJitter,\n",
        "                                PickFirstChannels)\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from itertools import product"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooJyRfjyJ-xB",
        "colab_type": "text"
      },
      "source": [
        "Check if the GPU is enabled.\n",
        "\n",
        "If not, then change CPU to GPU: Runtime -> Change runtime type -> Hardware accelerator -> GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNpfFe9BFHVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        " raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7rwkAQP3288",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_value = 42\n",
        "torch.manual_seed(seed_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzngHnLI35R4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Device_count:\", torch.cuda.device_count())\n",
        "print(\"Device_name:\",torch.cuda.get_device_name(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_G9zsHy38BF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda:0')\n",
        "print(f\"Computation device: {device}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn4wi_oC3-qj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# custom dataset\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, videos, labels=None, spatial_transform=None):\n",
        "        self.X = videos\n",
        "        self.y = labels\n",
        "        self.spatial_transform = spatial_transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return (len(self.X))\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        vr = VideoReader(self.X[i]) # Read video\n",
        "        bridge.set_bridge('native') # native output: <class 'decord.ndarray.NDArray'>, (240, 426, 3)\n",
        "        duration = len(vr) # Number of frames in video\n",
        "        frames = 16\n",
        "        steps = math.floor(duration/frames)\n",
        "        start_frame = 0\n",
        "        stop_frame = steps * frames\n",
        "        \n",
        "        frame_id_list = range(start_frame, stop_frame, steps) # positions of frames\n",
        "        \n",
        "        video_snippet = vr.get_batch(frame_id_list).asnumpy() # Will get a batch of 16 frames from video\n",
        "        clip = []\n",
        "        # Transform into images\n",
        "        for img in video_snippet:\n",
        "            im_pil = Image.fromarray(img)\n",
        "            clip.append(im_pil)\n",
        "\n",
        "        # Spatial transform on images\n",
        "        if self.spatial_transform is not None:\n",
        "            clip = [self.spatial_transform(img) for img in clip]\n",
        "\n",
        "        clip = torch.stack(clip, 0).permute(1, 0, 2, 3) # [Batch, Channel, Depth, Height, Width]\n",
        "   \n",
        "        label = self.y[i]\n",
        "        \n",
        "        # Video (stacked frames): torch.Size([1, 3, 16, 112, 112])\n",
        "        # Label: torch.Size([1])\n",
        "        return (clip.clone().detach().requires_grad_(True), torch.tensor(label, dtype=torch.long))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgVROiw8wUkl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RunBuilder():\n",
        "    @staticmethod\n",
        "    def get_runs(params):\n",
        "\n",
        "        Run = namedtuple('Run', params.keys())\n",
        "\n",
        "        runs = []\n",
        "        for v in product(*params.values()):\n",
        "            runs.append(Run(*v))\n",
        "\n",
        "        return runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eezGYY_E4Cq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate resnet model (34, 50)\n",
        "# K - Kinetics-700\n",
        "# KM - Kinetics-700 and Moments in Time\n",
        "model_architecture = 50\n",
        "dataset = 'KM'\n",
        "\n",
        "model = ResNet.initialize_model(model_architecture=model_architecture, model_dataset=dataset, num_classes=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqsYyM-M4FCi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Freeze/Unfreeze layers\n",
        "for name, child in model.named_children():\n",
        "    if name in ['layer4', 'fc']: # Layer that will be unfrozen\n",
        "        print(name + ' is unfrozen')\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = True\n",
        "    else:\n",
        "        print(name + ' is frozen')\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = False  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq_CJ7Ho4H3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.to(device) # Network on the GPU\n",
        "model.eval() # eval mode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jovhYlIc4LNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"{total_params:,} total parameters.\")\n",
        "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"{total_trainable_params:,} training parameters.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxFW2A-V4PyW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training function\n",
        "def fit(model, train_dataloader):\n",
        "    print('Training')\n",
        "    model.train() # training mode activated if no_grad() have deactivate the gradient calculation part in validation function\n",
        "    train_running_loss = 0.0\n",
        "    train_running_correct = 0\n",
        "    for i, data in tqdm(enumerate(train_dataloader), total=int(len(train_data)/train_dataloader.batch_size)):\n",
        "        data, target = data[0].to(device), data[1].to(device) \n",
        "        optimizer.zero_grad() # Reset optimizer to zero otherwise it will accumulate all the gradients\n",
        "        outputs = model(data) # Input the bathed images to the model to get a output (prediction)\n",
        "        \n",
        "        # From the loss function we will get back a loss tensor. PyTorch have the computaional graph for the tensor that will be used in the backpropagation step \n",
        "        loss = criterion(outputs, target) # calculate the loss from the loss/error function (prediction_label - true_label)\n",
        "        train_running_loss += loss.item() # new loss value to update the current training loss value\n",
        "        _, preds = torch.max(input=outputs.data, dim=1) # Returns the maximum value of all elements in the input tensor\n",
        "        train_running_correct += (preds == target).sum().item() # Count the right numbers of correct prediction\n",
        "        loss.backward() # Calculate gradients\n",
        "        optimizer.step() # Update the weights\n",
        "        \n",
        "    train_loss = train_running_loss/len(train_dataloader.dataset)\n",
        "    train_accuracy = 100. * train_running_correct/len(train_dataloader.dataset)\n",
        "    \n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}\")\n",
        "    \n",
        "    return train_loss, train_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYRZ0AqZ4RAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#validation function\n",
        "def validate(model, test_dataloader):\n",
        "    print('Validating')\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    val_running_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for i, data in tqdm(enumerate(test_dataloader), total=int(len(test_data)/test_dataloader.batch_size)):\n",
        "            data, target = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, target)\n",
        "            \n",
        "            val_running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            val_running_correct += (preds == target).sum().item()\n",
        "        \n",
        "        val_loss = val_running_loss/len(test_dataloader.dataset)\n",
        "        val_accuracy = 100. * val_running_correct/len(test_dataloader.dataset)\n",
        "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}')\n",
        "        \n",
        "        return val_loss, val_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mtl2DLAGO1pw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if (dataset == 'K'):\n",
        "  mean = [0.4345, 0.4051, 0.3775]\n",
        "  std = [0.2768, 0.2713, 0.2737]\n",
        "else:\n",
        "  mean = [0.5, 0.5, 0.5]\n",
        "  std = [0.5, 0.5, 0.5]\n",
        "\n",
        "sample_size = 112 # resolution of frame\n",
        "\n",
        "# MultiScaleCornerCrop (four-corner cropping)\n",
        "scales = [1.0]\n",
        "scale_step = 1 / (2**(1 / 4))\n",
        "\n",
        "for _ in range(1, 5):\n",
        "    scales.append(scales[-1] * scale_step)\n",
        "\n",
        "spatial_transform_train = Compose([\n",
        "                                   MultiScaleCornerCrop(sample_size, scales),\n",
        "                                   RandomHorizontalFlip(),\n",
        "                                   ToTensor(),\n",
        "                                   Normalize(mean, std)\n",
        "                                   ])\n",
        "\n",
        "spatial_transform_validation =  Compose([\n",
        "                                        Resize(sample_size),\n",
        "                                        CenterCrop(sample_size),\n",
        "                                        ToTensor(),\n",
        "                                        Normalize(mean, std)\n",
        "                                        ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPZIf-14WSN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the data.csv file and get the video paths and labels\n",
        "df = pd.read_csv('./fight_recognition/input/data.csv')\n",
        "X = df.video_path.values # video paths\n",
        "y = df.target.values # targets\n",
        "\n",
        "# Split into training and valtidation/test dataset\n",
        "(xtrain, xtest, ytrain, ytest) = train_test_split(X, y, test_size=0.20, random_state=seed_value)\n",
        "\n",
        "print(f\"Training videos: {len(xtrain)}\")\n",
        "print(f\"Training labels: {len(ytrain)}\\n\")\n",
        "\n",
        "print(f\"Validation videos: {len(xtest)}\")\n",
        "print(f\"Validation labels: {len(ytest)}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKickt-V4TGN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test diffrent hyperparamters\n",
        "params = OrderedDict(\n",
        "    lr = [.001]\n",
        "    ,batch_size = [16, 32]\n",
        "    ,momentum = [.9]\n",
        ")\n",
        "\n",
        "for run in RunBuilder.get_runs(params):\n",
        "  ResNet = model\n",
        "\n",
        "  # Create training and testing dataset\n",
        "  train_data = VideoDataset(xtrain, ytrain, spatial_transform_train)\n",
        "  test_data = VideoDataset(xtest, ytest, spatial_transform_validation)\n",
        "\n",
        "  TrainLoader = DataLoader(train_data, batch_size=run.batch_size, shuffle=True) # If shuffle is set to True, it will have the data reshuffled at every epoch\n",
        "  TestLoader = DataLoader(test_data, batch_size=run.batch_size, shuffle=False)\n",
        "\n",
        "  optimizer = optim.SGD([{'params': ResNet.layer4.parameters()}, \n",
        "                        {'params': ResNet.fc.parameters(), 'lr': run.lr}\n",
        "                          ], lr=run.lr*1e-2,momentum=run.momentum, weight_decay=0.0001)\n",
        "\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # SGDR\n",
        "  t_mult = 1 # Cycle multiplication\n",
        "  t_max = 20 # Maximum number of iterations/epochs\n",
        "  scheduler = CosineAnnealingLR_with_Restart(optimizer, T_max=t_max, T_mult=t_mult, model=ResNet,\n",
        "                                             out_dir='./fight_recognition/outputs/snapshots/',\n",
        "                                             take_snapshot=True,\n",
        "                                             eta_min=1e-9)\n",
        "\n",
        "  comment = f'-{run}' # Comments in Tensorboard for each run\n",
        "  current_time = datetime.now().strftime(\"Date_%Y-%m-%d_Time_%H-%M-%S\")\n",
        "\n",
        "  log_dir = os.path.join(\n",
        "      'runs', \n",
        "      current_time + '_' + comment\n",
        "  )\n",
        "\n",
        "  tb = SummaryWriter(log_dir=log_dir) # In \"runs\" folder\n",
        "\n",
        "  epochs = 10\n",
        "  start = time.time()\n",
        "  for epoch in range(epochs):\n",
        "      scheduler.step() # SGDR\n",
        "      print(f\"Epoch {epoch+1} of {epochs}\")\n",
        "      train_epoch_loss, train_epoch_accuracy = fit(ResNet, TrainLoader) # Train network\n",
        "      val_epoch_loss, val_epoch_accuracy = validate(ResNet, TestLoader) # Validate network on test/validation dataset\n",
        "\n",
        "      # Add to Scalar in Tensorboard\n",
        "      tb.add_scalar(\"Training Loss\", train_epoch_loss * run.batch_size, epoch)\n",
        "      tb.add_scalar(\"Valifation Loss\", val_epoch_loss * run.batch_size, epoch)\n",
        "      tb.add_scalar(\"Training accuracy\", train_epoch_accuracy, epoch)\n",
        "      tb.add_scalar(\"Valifation accuracy\", val_epoch_accuracy, epoch)\n",
        "\n",
        "  end = time.time()\n",
        "\n",
        "  tb.close()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  print(f\"{(end-start)/60:.3f} minutes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcyK_PjID49H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}